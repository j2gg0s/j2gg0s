请允许我的思绪发散下.

上周在寻找分析型数据库的时候, 又遇到了两个熟悉但不认识的词: 湖仓(data lake) 和数仓(data warehouse).
data lake 是指多个数据源的结构化数据或者非结构化数据被汇聚到同一个系统.
data warehouse 是指面向最终目的, 比如分析, 产生的结构化数据.

各种类型的存储查询系统有自己的优点和限制.
以 MySQL 为代表的关系型数据库, 支持事物, 支持数据的增删查改.
以 ElasticSearch 为代表的搜索引擎, 支持针对文本的模糊搜索.
以及针对海量数据的分析型数据库.

并没有一个系统能满足所有的需求, 所以我们不得不将数据在各个系统之间转移.
于是, Data Transmission Service 成为了整个系统中不可或缺的一部分.

## 数据同步的关键
以将 MySQL 的数据同步迁移到 ElasticSearch 为例.
成熟的方案是 source 通过 MySQL 的 binlog 将数据变更同步到 Kafka, sink 在消费 Kakfa 中消息将数据写入到 ElasticSearch.

为什么大家最终都选择了类似的方案呢?

我们先考虑数据同步中的两个关键指标: 数据的完整性和同步的速率.

我们首先引入消息消费中的一个概念: 至多一次(at most once), 至少一次(at least once) 和正好一次(exactly one).
我们基本没有办法在数据同步中保证正好一次, 或者为了实现正好一次, 我们需要承担天价.
那么为了保证同步数据的完整性, 我们必须要选择至少一次. 幸好大多数存储系统支持 upsert 的语义, 所以重复插入并不会带来致命错误.

基于 binlog 的特性, 我们可以简单的通过: 消费, 写入成功后再消费下一条的方式来实现至少一次.

为什么要通过 Kafka 来中转而不是直接写入 ES 呢?
一方面, 需要消费 MySQL 数据的有多方, Kafka 的吞吐和扩展性都比 MySQL 好, 所以可以有 source 将数据变更同步到 Kafka 后由多方消费.
另一方面, 需要写入 ElasticSearch 的数据可能来自多个上游, 通过 Kafka 中转并约定格式后, sink 可以不用考虑不同的数据来源和格式.

引入 Kafka 的另一个收益是更容易保证同步的速率.
绝大多系统在批量写入时可以达到更高的吞吐, 使用 Kafka 聚拢数据可以更好&更简单的实现批量.

## Kafka Connect
业务系统的开发类似于搭建积木, 关键是选择合适的积木以巧妙的方式搭建符合业务需求的建筑.
大多数时候不需要自己去创建积木, 作坊的产出质量一般不会比久经考验的成品更高.

Kafka 有比较成熟的生态来实现 DTS: Kafka Connect.

Connect 是在 Kafka 之上构建的一套生态体系.
source 负责将数据从其他系统导入到 Kafka,
sink 负责将数据从 Kafka 导入到其他系统.
官方或三方根据定义的接口实现 source/sink, Connect 根据用户配置, 动态加载这些 plugin, 并负责其运行.
